####################@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@##################
------查看网络是否能ping通
nc -z 10.7.5.11 5432

sed -i '$a ]' CustPackageInstHis.json.bak
sed -i '1i [' CustPackageInstHis.json.bak

mysql -A datasupport -h 10.7.4.79 -uroot -pzt9YdPveyA9UDPKJGKJ -e "select sql_str from sql_create_table" > sql_create_table.sql
su - gpadmin -c "psql -h 10.7.5.16 -p 5432 -U gpadmin -d bigdata -f /home/bigdata/sql_create_table.sql" > sql_create_table.log

java -jar test.jar para1 para2 para3

iostat -x -k -d 1
iostat -x 1

通过标准输入输出实现Greenplum文件导入导出
-----导出
psql -h 192.168.0.1 -U gpadmin bigdata -c "copy(select * from src.dis_geocoding_celllocation) to STDOUT;" >/root/dis_geocoding_celllocation.txt
-----导入
cat /mytemp/test.txt |psql -h 192.168.0.1 -U gpadmin bigdata -c 'copy src.dis_geocoding_celllocation from stdin;'

ps -ef | grep tomcat | grep -v grep | awk '{print "kill -9 " $2}'|sh
chown gpadmin t_term_ssim_estfail.csv
chgrp gpadmin  t_term_ssim_estfail.csv

tar -zcvf data_perflog.tar.gz data_perflog
tar -zxvf /usr/local/test.tar.gz

#######此ERROR的意思是csv文件中存在这样一行记录，此记录的大小超过了gpfdist一行记录大小的默认值32KB，可在启动gpfdist服务时添加-m参数解决此问题
##//在此设置了gpfdist一行记录大小为10MB
nohup gpfdist -d $path -p $port -l $log -m 10485760 &

more localhost_access_log.2018-08-07.txt|awk '{print $7}'|grep -v js|grep -v css|grep -v getDBDumpMsg|grep -v png|grep -v gif|grep -v ico|sort

more localhost_access_log.2018-08-*|grep "cgReportController.do?datagrid"|awk '{print $7}'|sort -n|uniq -c |sort -k1,1nr | head -20

curl https://bigdataplatform.ucloudlink.com/dap/ 用这个命令在17上面执行。如果很慢没有返回，就是进程出问题了。如果很快返回，但是在本机很慢，那有可能是网络问题

####################@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@##################
#!/bin/bash

###########################mysql_datasupport###############################
v_mysql_db='datatest'
v_mysql_ip='10.1.1.1'
v_mysql_u='root'
v_mysql_p='123456'
v_mysql_str="SELECT table_name,real_table_name FROM md_table_conf WHERE db_id=1 AND is_all='add' AND STATUS=1;"
v_url_file='/uCloudlink/greenplum/self_check/get_tablename.cfg'
mysql -A ${v_mysql_db} -h ${v_mysql_ip} -u${v_mysql_u} -p${v_mysql_p} -e "${v_mysql_str}" | sed '1d' > ${v_url_file}

##########################mongodb############################
v_mongo_db='db_user'
v_mongo_ip='10.1.4.4'
v_mongo_u='mongotest'
v_mongo_p='123456'
v_mongo_port='27019'


cat $v_url_file | while read line
        do
				v_line_t=`echo $line|awk '{print $1}'`
				v_line_p=`echo $line|awk '{print $2}'`		
				echo $v_line
				v_table_str="SELECT tablename FROM t_source_record WHERE partition_tablename='"${v_line_p}"';"
				v_table_name="`mysql -A ${v_mysql_db} -h ${v_mysql_ip} -u${v_mysql_u} -p${v_mysql_p} -e \"${v_table_str}\" | sed '1d'`"
				v_execute="$v_table_name"
				v_mongo="`mongo ${v_mongo_db} --port ${v_mongo_port} -u ${v_mongo_u} -p ${v_mongo_p}  --authenticationDatabase=${v_mongo_db} --host ${v_mongo_ip} --eval \"db.${v_line_p}.stats().count\" | sed -n '3p'`"
				v_mongo_count="$v_mongo"              #############--eval命令行查询mongodb有多少行数据
				echo $v_mongo_count
				v_datetime=`date -d today +"%Y-%m-%d %T"`
				if [ -z "$v_execute" ] ; then
					v_insert="INSERT INTO t_source_record(systemname,tablename,partition_tablename,countrows,update_time) VALUES('oss_perflog','${v_line_t}','${v_line_p}',${v_mongo_count},'${v_datetime}');"
					mysql -A ${v_mysql_db} -h ${v_mysql_ip} -u${v_mysql_u} -p${v_mysql_p} -e "${v_insert}" 
					echo "Insert "
				else
					v_update="UPDATE t_source_record SET countrows=${v_mongo_count},update_time='${v_datetime}' WHERE partition_tablename='${v_line_p}';";
					mysql -A ${v_mysql_db} -h ${v_mysql_ip} -u${v_mysql_u} -p${v_mysql_p} -e "${v_update}"
					echo "Update"
				fi
        done
		
		
####################@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@##################
#!/bin/sh
#重写DUMP脚本
echo '#!/bin/sh' > dump_oss_perflog_real.sh
url_dump=/data/04_ETL_souredata_to_gpdata_oss_perflog_encrypt_real/file_oss_perflog_real.sh
cat $url_dump | while read line
do  
        table_name=`echo ${line}|awk -F '-c' '{print $2}' | awk -F '-f' '{print $1}'`
        echo 'begin_time=`date "+%Y-%m-%d %H:%M:%S"`' >> dump_oss_perflog_real.sh
        echo "echo \"表${table_name}开始时间:\"\${begin_time}" >> dump_oss_perflog_real.sh
        echo 'echo ${begin_time}' >> dump_oss_perflog_real.sh
        echo ${line} >> dump_oss_perflog_real.sh
        echo 'end_time=`date "+%Y-%m-%d %H:%M:%S"`' >> dump_oss_perflog_real.sh
        echo 'echo ${end_time}' >> dump_oss_perflog_real.sh
        echo "echo \"表${table_name}结束时间:\"\$end_time" >> dump_oss_perflog_real.sh
        echo "file_size=\"du -sh `echo ${line}|awk -F '-o' '{print \$2}'` | awk '{print \$1}'\"" >> dump_oss_perflog_real.sh
        echo "file_size=\`eval \${file_size} | awk '{print \$1}'\`" >> dump_oss_perflog_real.sh
        echo 'time_diff=$(($(date +%s -d "${end_time}") - $(date +%s -d "${begin_time}")))' >> dump_oss_perflog_real.sh
        echo "echo \"表${table_name}执行时间:\"\${time_diff}s" >> dump_oss_perflog_real.sh
        echo "if [ \"\${time_diff}\" -gt \"60\" ];then
	echo \"dump table cost gt \${time_diff}s,table_name is:${table_name},size is:\${file_size}\"
fi" >> dump_oss_perflog_real.sh
done

####################@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@##################
#######################################多线程DUMP###############################
#!/bin/sh

url_dump=/data/04_ETL_souredata_to_gpdata_bss_crm_encrypt/file_bss_crm.sh
dump_count=`wc -l  ${url_dump}|awk '{print $1}'`
batch=10
loop_count=$(expr $dump_count / $batch)
v_cmp=$(($batch * $loop_count))
if [ "$v_cmp" -ne "$dump_count" ];then
	loop_count=$(($loop_count + 1))
fi
echo "loop_count is: $loop_count"
for ((i=0; i<$loop_count;))
do
	start_num=$(expr $i \* $batch + 1)
	echo $start_num
	end_num=$((($i+1) * $batch))
	echo $end_num
	for ((j=$start_num; j<=$end_num;))
	do
		
		v_tablename=`sed -n "${j}p" ${url_dump}|awk -F '-c' '{print $2}' | awk -F '-f' '{print $1}'`
		begin_time=`date "+%Y-%m-%d %H:%M:%S"`
		echo "表 ${v_tablename} 开始时间:"${begin_time}
		v_dump=`sed -n "${j}p" ${url_dump}`
		if [ ! -n "${v_dump}" ];then
			break
		fi
		echo ${v_dump}
		eval ${v_dump}
		end_time=`date "+%Y-%m-%d %H:%M:%S"`
		echo "表 ${v_tablename} 结束时间:"$end_time
		file_size="du -sh `echo ${v_dump}|awk -F '-o' '{print $2}'` | awk '{print $1}'"
		file_size=`eval ${file_size} | awk '{print $1}'`
		time_diff=$(($(date +%s -d "${end_time}") - $(date +%s -d "${begin_time}")))
		echo "表 ${v_tablename} 执行时间:"${time_diff}s
		if [ "${time_diff}" -gt "2" ];then
				echo "dump table cost ${time_diff}s,table_name is: ${v_tablename} ,size is:${file_size}"
		fi	
		j=$(expr $j + 1)
	done &

	i=$(expr $i + 1)
done

####################@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@##################
#######################################多线程DUMP 第二种方法###############################
#!/bin/sh
dump_starttime=`date "+%Y-%m-%d %H:%M:%S"`
echo "dump开始时间:"$dump_starttime
url_dump=/data/04_ETL_souredata_to_gpdata_oss_perflog_encrypt_real/file_oss_perflog_real.sh
all_num_string="`cat /data/04_ETL_souredata_to_gpdata_oss_perflog_encrypt_real/file_oss_perflog_real.sh | wc -l`"
all_num=$all_num_string
echo $all_num
thread_num=3
echo $thread_num
tmp_fifofile="/tmp/$.fifo"
mkfifo $tmp_fifofile
exec 6<>$tmp_fifofile
rm $tmp_fifofile
for ((i=1;i<=${thread_num};i++))
do
{
echo
}
done >&6
for num in `seq 1 ${all_num}`
do
{
read -u6
{
table_name="sed -n '${num}p' ${url_dump} | awk -F '-c' '{print \$2}' | awk -F '-f' '{print \$1}'"
table_name=`eval ${table_name}`
echo ${table_name}
begin_time=`date "+%Y-%m-%d %H:%M:%S"`
echo "表${table_name}开始时间:"${begin_time}
echo ${begin_time}
dump_commond="sed -n '${num}p' ${url_dump}"
dump_commond=`eval ${dump_commond}`
echo $dump_commond
eval $dump_commond
end_time=`date "+%Y-%m-%d %H:%M:%S"`
echo ${end_time}
echo "表结束时间:"$end_time
table_road="sed -n '${num}p' ${url_dump}|awk -F '-o' '{print \$2}'"
table_road=`eval $table_road`
file_size="du -sh $table_road | awk '{print \$1}'"
echo $file_size
file_size=`eval ${file_size} | awk '{print $1}'`
time_diff=$(($(date +%s -d "${end_time}") - $(date +%s -d "${begin_time}")))
echo "表执行时间:"${time_diff}s
if [ "${time_diff}" -gt "2" ];then
        echo "dump table cost ${time_diff}s,table_name is:${table_name},size is:${file_size}"
fi
echo >&6
}&
}
done
wait
exec 6>&-
dump_endtime=`date "+%Y-%m-%d %H:%M:%S"`
echo "dump结束时间:"$dump_endtime

####################@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@##################
####################################S3#############################################
#!/bin/bash

batch_no=`date +%Y%m%d`

######

       cd $1/add/$2/
mysql -A $7 -h $8 -u$9 -p${10} -e "select table_name from job_src2src_conf t inner join md_table_conf s on t.source_tables_id=s.id and s.is_all='add' and s.status=1 where t.job_conf_id=1;">readfile

###

    encrypt_start=`date +%s`
cat readfile | tail -n +2|while read LINE
do
file_encrypt_start=`date +%s`
filename=$LINE.csv
file_size=`du -h $filename|awk '{print $1}'`
openssl enc -$3 -a -salt -in ./$filename -out $4/add/$filename -k $5
file_encrypt_ut=$((`date +%s`-$file_encrypt_start))
mysql -A $7 -h $8 -u$9 -p${10} -e "delete from datasupport.s3_upload_job_log_detail where file_name='$filename' and job_code='$2' and batch_no='$batch_no';insert into datasupport.s3_upload_job_log_detail (job_code,batch_no,file_name,file_size,encrypt_ut)values('$2','$batch_no','$filename','$file_size',$file_encrypt_ut);"
done
encrypt_ut=$((`date +%s`-$encrypt_start))


###scp
scp_start=`date +%s`
cat readfile | tail -n +2|while read LINE
do
file_scp_start=`date +%s`
filename=$LINE.csv
scp -r $4/add/$filename huangkaihao@1.7.5.2:$6/add/
file_scp_ut=$((`date +%s`-$file_scp_start))
mysql -A $7 -h $8 -u$9 -p${10} -e "update datasupport.s3_upload_job_log_detail set scp_ut=$file_scp_ut where file_name='$filename' and job_code='$2' and batch_no='$batch_no';"
done
scp_ut=$((`date +%s`-$scp_start))

mysql -A $7 -h $8 -u$9 -p${10} -e "delete from datasupport.s3_upload_job_log where job_code='$2' and batch_no='$batch_no';insert into datasupport.s3_upload_job_log (job_code,batch_no,encrypt_ut,scp_ut)values('$2','$batch_no',$encrypt_ut,$scp_ut);"

######ȫcd $1/all/$2/
ls *.csv>readfile
cat readfile|while read LINE
do
openssl enc -$3 -a -salt -in ./$LINE -out $4/all/$LINE -k $5
done
scp -r $4/all/. huangkaihao@1.7.5.2:$6/all/

####################@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@##################
####################################S3#############################################
#!/bin/bash
ssh huangkaihao@1.7.5.2 -tt<<remotessh
/usr/local/bin/expect <<-EOF
set time 30
spawn aws configure
expect "AWS Access Key ID*"
send "$1\r"
expect "AWS Secret Access Key*"
send "$2\r"
expect "Default region name*"
send "$3\r"
expect "Default output format*"
send "$4\r"
expect eof
EOF
exit
remotessh
####################@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@##################
####################################S3#############################################
#!/bin/bash
v_yyyy=`date +%Y`
v_mm=`date +%m`
v_dd=`date +%d`
v_hh=`date +%H`
v_week=`date +%W`
batch_no=$v_yyyy$v_mm$v_dd

######

       s3_start=`date +%s`
mysql -A $4 -h $5 -u$6 -p$7 -e "select table_name from job_src2src_conf t inner join md_table_conf s on t.source_tables_id=s.id and s.is_all='add' and s.status=1 where t.job_conf_id=1;">readfile
cat readfile | tail -n +2|while read LINE
do
file_s3_start=`date +%s`
tablename=$LINE
filename=$LINE.csv
file_path=$2/$3/$tablename/$v_yyyy/$v_mm/$v_dd/
v_sql="delete from datasupport.file_encrypt_info where table_name='$tablename' and batch_no='$batch_no';insert into datasupport.file_encrypt_info(system_code,table_name,file_path,encrypt_method,keyt,batch_no,save_method)values ('$3','$tablename','$file_path','$8','$9','$batch_no','add');"
ssh -n huangkaihao@1.7.5.2 aws s3 cp $1/add/$filename $file_path
mysql -A $4 -h $5 -u$6 -p$7 -e "${v_sql}"
file_s3_ut=$((`date +%s`-$file_s3_start))
mysql -A $4 -h $5 -u$6 -p$7 -e "update datasupport.s3_upload_job_log_detail set s3_ut=$file_s3_ut where file_name='$filename' and job_code='$3' and batch_no='$batch_no';"
done
s3_ut=$((`date +%s`-$s3_start))
mysql -A $4 -h $5 -u$6 -p$7 -e "update datasupport.s3_upload_job_log set s3_ut=$s3_ut where job_code='$3' and batch_no='$batch_no';"


######ȫssh huangkaihao@1.7.5.2 ls $1/all/*.csv>readfile
cat readfile|while read LINE
do
filename=$(basename $LINE .csv)
batch_no=$v_yyyy$v_week
file_path=$2/$3/$filename/$v_yyyy/$v_week/
v_sql="delete from datasupport.file_encrypt_info where table_name='$filename' and batch_no='$batch_no';insert into datasupport.file_encrypt_info(system_code,table_name,file_path,encrypt_method,keyt,batch_no,save_method)values ('$3','$filename','$file_path','$8','$9','$batch_no','all');"
ssh -n huangkaihao@1.7.5.2 aws s3 cp $LINE $file_path
mysql -A $4 -h $5 -u$6 -p$7 -e "${v_sql}"


####################@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@##################
####################################备份backup#############################################

#!/bin/bash

psql -h 10.7.5.16 -U gpadmin bigdata -c "copy(select * from ods.${1} where createdate>='2018-03-01 00:00:00' and createdate <'2018-04-01 00:00:00'::timestamp) to STDOUT with csv header;" > /uCloudlinkSSD/dumpdata/data_bak/oss_perflog/${1}.csv
filename=${1}.csv
file_path=s3://s3-ukl-business-bigdata-data001/oss_perflog/$1/2018/010203/
batch_no=201801~03
cd /uCloudlinkSSD/dumpdata/data_bak/oss_perflog
openssl enc -des3 -a -salt -in ./$filename -out /uCloudlinkSSD/dumpdata/data_bak/encrypt_temp/$filename -k 10b2eb3819ceacfeff0fbf303bb3faf7
scp -r /uCloudlinkSSD/dumpdata/data_bak/encrypt_temp/$filename huangkaihao@10.7.5.27:/uCloudlink/s3data/s3bakup/
ssh -n huangkaihao@10.7.5.27 aws s3 cp /uCloudlink/s3data/s3bakup/$filename $file_path
mysql -A datasupport -h 10.7.4.79 -uroot -pzt9YdPveyA9UDPKJGKJ -e "insert into datasupport.file_encrypt_info(system_code,table_name,file_path,encrypt_method,keyt,batch_no,save_method)values ('oss_perflog','$1','$file_path','des3','10b2eb3819ceacfeff0fbf303bb3faf7','$batch_no','add');"
rm -f /uCloudlinkSSD/dumpdata/data_bak/oss_perflog/$filename
rm -f /uCloudlinkSSD/dumpdata/data_bak/encrypt_temp/$filename
ssh -n huangkaihao@10.7.5.27 rm -f /uCloudlink/s3data/s3bakup/$filename

####################@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@##################
####################################备份backup#############################################
#!/bin/sh

###########################mysql###############################

v_mysql_db='datasupport'
v_mysql_ip='10.7.7.7'
v_mysql_u='root'
v_mysql_p='zt9YdPveyA9UDPKJGKJ'
i=2
v_mysql_str="SELECT
        table_name
FROM
        job_src2src_conf
JOIN md_table_conf ON source_tables_id = md_table_conf.id
WHERE
        job_conf_id = '1' and md_table_conf.status='1' and is_all='add';"


real_table="`mysql -A ${v_mysql_db} -h ${v_mysql_ip} -u${v_mysql_u} -p${v_mysql_p} -e "${v_mysql_str}"`"
echo $real_table
table_names=($real_table)
unset table_names[0]
echo ${table_names[@]}

v_curday=`date +%Y%m%d`
v_curdayhour=`date +%Y%m%d%H`
v_15agoday=`date -d -5day +%Y%m%d`

if [ -d /uCloudlink/dumpdata/data_bak/oss_perflog/all/$v_curday/$v_curdayhour ];then
   rm -drf /uCloudlink/dumpdata/data_bak/oss_perflog/all/$v_curday/$v_curdayhour
   mkdir -p /uCloudlink/dumpdata/data_bak/oss_perflog/all/$v_curday/$v_curdayhour
   mv /data/dumpdata/metadata/all/oss_perflog  /uCloudlink/dumpdata/data_bak/oss_perflog/all/$v_curday/$v_curdayhour
else
   mkdir -p /uCloudlink/dumpdata/data_bak/oss_perflog/all/$v_curday/$v_curdayhour
   mv /data/dumpdata/metadata/all/oss_perflog  /uCloudlink/dumpdata/data_bak/oss_perflog/all/$v_curday/$v_curdayhour
fi

if [ -d /uCloudlink/dumpdata/data_bak/oss_perflog/all/$v_15agoday ];then
   rm -drf /uCloudlink/dumpdata/data_bak/oss_perflog/all/$v_15agoday
fi
mkdir -p /data/dumpdata/metadata/all/oss_perflog

if [ -d /uCloudlink/dumpdata/data_bak/oss_perflog/add/$v_curday/$v_curdayhour ];then
   rm -drf /uCloudlink/dumpdata/data_bak/oss_perflog/add/$v_curday/$v_curdayhour
   mkdir -p /uCloudlink/dumpdata/data_bak/oss_perflog/add/$v_curday/$v_curdayhour
   for table_name in ${table_names[@]}
        do
                #table_name=`echo $table_names | cut -d ' ' -f "$i"`
                echo $table_name
                mv /data/dumpdata/metadata/add/oss_perflog/${table_name}.csv  /uCloudlink/dumpdata/data_bak/oss_perflog/add/$v_curday/$v_curdayhour
                
        done
else
   mkdir -p /uCloudlink/dumpdata/data_bak/oss_perflog/add/$v_curday/$v_curdayhour
   for table_name in ${table_names[@]}
        do
                #table_name=`echo $table_names | cut -d ' ' -f "$i"`
                echo $table_name
                mv /data/dumpdata/metadata/add/oss_perflog/${table_name}.csv  /uCloudlink/dumpdata/data_bak/oss_perflog/add/$v_curday/$v_curdayhour
                
        done
fi

if [ -d /uCloudlink/dumpdata/data_bak/oss_perflog/add/$v_15agoday ];then
   rm -drf /uCloudlink/dumpdata/data_bak/oss_perflog/add/$v_15agoday
fi
mkdir -p /data/dumpdata/metadata/add/oss_perflog


####################@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@##################
####################################调用dw存储过程#############################################

#!/bin/bash

source /etc/profile

##########################greenplum############################
v_gp_db='bigdata'
v_gp_ip='10.7.7.6'
v_gp_u='gpadmin'
v_gp_port='5432'

##############################################调用dw存储过程#################################################################################
begintime=`date "+%Y-%m-%d %H:%M:%S"`
current_day=`date "+%Y-%m-%d"`
echo "按天发货归还统计dw_oms_deliver_return_day开始时间:${begintime}"
v_gp_dw_oms_deliver_return_day="SELECT dw.dw_oms_deliver_return_day('$current_day'::DATE);"
v_dw_oms_deliver_return_day="`su - gpadmin -c \"psql -h ${v_gp_ip} -p ${v_gp_port} -U ${v_gp_u} -d ${v_gp_db} -c \\\"${v_gp_dw_oms_deliver_return_day}\\\"\" | sed -n '3p'`"
v_gp_stored_dw_oms_deliver_return_day="$v_dw_oms_deliver_return_day"
end_times=$(date "+%Y-%m-%d %H:%M:%S")
echo "按天发货归还统计dw_oms_deliver_return_day结束时间:${end_times}"

####################@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@##################
####################################特殊流程#############################################
year=$(date +%Y)
month=$(date +%m)
premonth=$(date -d "$(date +%Y%m)01 last month" +%m)
premonth=$(printf "%02d" $premonth)
day="01"
echo $year$premonth$day

#####################计算昨日0点到今日0点时间戳
now_day=$(date  +%Y%m%d )
now_d_unix=`expr $(date -d $now_day  +%s) \* 1000`
yestoday_day=$(date -d last-day  +%Y%m%d) 
yestoday_day01=$(date -d last-day  +%Y-%m-%d)
yestoday_d_unix=`expr $(date -d $yestoday_day +%s) \* 1000`
echo $now_d_unix
echo $yestoday_d_unix
#########################################################

v_datestamp=`date +%Y%m%d%H%M%S`
echo "开始时间:"$v_datestamp

if [ -f "/data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_history.csv" ];then
        mv /data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_history.csv /data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_history_bak/t_monitor_term_history_$v_datestamp.csv
fi

###################################mysql_datasupport 第一步更新分表###############################
v_mysql_db='datasupport'
v_mysql_ip='10.7.4.79'
v_mysql_u='root'
v_mysql_p='zt9YdPveyA9UDPKJGKJ'
v_mysql_str="UPDATE t_monitor_term_history  SET real_name  = (SELECT tt.collectionName FROM (SELECT b.collectionName,b.type  FROM (SELECT  MAX(endtime) endtime,TYPE FROM t_db_sub_collections_oss_system WHERE FROM_UNIXTIME(beginTime/1000)<DATE_ADD(NOW(),INTERVAL -1 DAY) AND FROM_UNIXTIME(endTime/1000)>DATE_ADD(NOW(),INTERVAL -1 DAY) AND TYPE='t_monitor_term_history' GROUP BY TYPE) a INNER JOIN t_db_sub_collections_oss_system b ON a.endtime=b.endtime AND a.type=b.type AND b.type='t_monitor_term_history') tt WHERE tt.type = table_name);"
mysql -A ${v_mysql_db} -h ${v_mysql_ip} -u${v_mysql_u} -p${v_mysql_p} -e "${v_mysql_str}"


#################################mongodb 第二步拼成mongodb 卸数语句###############################
v_mongo_db='oss_system'
v_mongo_ip='10.7.19.19'
v_mongo_u='xian_query'
v_mongo_p='HyyBjiyzmq20180327'
v_mongo_port='27019'
v_mongo_command=/data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_history_export.sh

v_table_name="`mysql -A ${v_mysql_db} -h ${v_mysql_ip} -u${v_mysql_u} -p${v_mysql_p} -e \"SELECT real_name FROM t_monitor_term_history WHERE table_name='t_monitor_term_history';\" | sed '1d'`"
v_maxtime="`mysql -A ${v_mysql_db} -h ${v_mysql_ip} -u${v_mysql_u} -p${v_mysql_p} -e \"SELECT maxtime FROM t_monitor_term_history WHERE table_name='t_monitor_term_history';\" | sed '1d'`"
echo $v_table_name
echo $v_maxtime

######################################## t_monitor_term_online 卸数装数#################################
v_datestamp_online=`date +%Y%m%d%H%M%S`
if [ -f "/data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_online.csv" ];then
        mv /data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_online.csv /data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_history_bak/t_monitor_term_online_$v_datestamp_online.csv
fi
v_execute_online="`/uCloudlink/mongodb3011/bin/mongoexport  -h ${v_mongo_ip} --port ${v_mongo_port} -u ${v_mongo_u} -p ${v_mongo_p} -d ${v_mongo_db} -c t_monitor_term_online -f _id,cardDownFlow,cardUpFlow,cellId,connectUserMax,devicetype,imei,imeis,imsi,iso2,isonline,lac,lastSeen,lock,logindatetime,mcc,mnc,process,sessionid,sids,softversion,sumFlow,sysDownFlow,sysUpFlow,t_mvnoId,t_orgId,totalFlow,uploadTime,userCode,userDownFlow,userGrade,userType,userUpFlow,country,loginCustomerId,mvnoId,orgId,u_mvnoName,u_orgName,sidList,backPlankSid,sids.XX.date,sids.XX.imsi,sids.XX.sidList,sids.XX.type,network,buType,powerLeft,city,province,sids.XX.switchCause,userGroup  --type=csv -o /data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_online.csv`"

echo $v_execute_online >> /data/dumpdata/data_bak/oss_system/t_monitor_term_history/oss_system_t_monitor_term_history.log

gpload -f /data/dumpdata/data_bak/oss_system/t_monitor_term_history/oss_system_t_monitor_term_online.yml >> /data/dumpdata/data_bak/oss_system/t_monitor_term_history/oss_system_t_monitor_term_history.log


##**************************************抽取上个月一日分表中比当前时间大的数据**************************************##
v_table_name_last=t_monitor_term_history_$year$premonth$day
echo $v_table_name_last


v_table_name_last=t_monitor_term_history_$year$premonth$day
echo $v_table_name_last
v_execute_last="/uCloudlink/mongodb3011/bin/mongoexport -h ${v_mongo_ip} --port ${v_mongo_port} -u ${v_mongo_u} -p ${v_mongo_p} -d ${v_mongo_db} -c ${v_table_name_last} -f _id,cardDownFlow,cardUpFlow,cellId,connectUserMax,devicetype,imei,imeis,iso2,isonline,lac,lastSeen,lock,logindatetime,logoutdatetime,mcc,mnc,process,sessionid,sids,softversion,sumFlow,sysDownFlow,sysUpFlow,t_mvnoId,t_orgId,totalFlow,uploadTime,userCode,userDownFlow,userGrade,userType,userUpFlow,country,loginCustomerId,mvnoId,orgId,imsi,u_mvnoName,u_orgName,transToHisTime,sidList,backPlankSid,sids.XX.date,sids.XX.imsi,sids.XX.sidList,sids.XX.type,network,buType,powerLeft,province,city,sids.XX.switchCause -q '{\"logoutdatetime\":{\$gte:$yestoday_d_unix,\$lt:$now_d_unix}}' --type=csv -o /data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_history.csv"
echo $v_execute_last
echo $v_execute_last > $v_mongo_command
chmod +x $v_mongo_command
. $v_mongo_command >> /data/dumpdata/data_bak/oss_system/t_monitor_term_history/oss_system_t_monitor_term_history.log


gpload -f /data/dumpdata/data_bak/oss_system/t_monitor_term_history/oss_system_t_monitor_term_history.yml >> /data/dumpdata/data_bak/oss_system/t_monitor_term_history/oss_system_t_monitor_term_history.log
##******************************************************************************************************************##

v_datestamp_last=`date +%Y%m%d%H%M%S`
if [ -f "/data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_history.csv" ];then
        mv /data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_history.csv /data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_history_bak/t_monitor_term_history_$v_datestamp_last.csv
fi
v_execute="/uCloudlink/mongodb3011/bin/mongoexport -h ${v_mongo_ip} --port ${v_mongo_port} -u ${v_mongo_u} -p ${v_mongo_p} -d ${v_mongo_db} -c ${v_table_name} -f _id,cardDownFlow,cardUpFlow,cellId,connectUserMax,devicetype,imei,imeis,iso2,isonline,lac,lastSeen,lock,logindatetime,logoutdatetime,mcc,mnc,process,sessionid,sids,softversion,sumFlow,sysDownFlow,sysUpFlow,t_mvnoId,t_orgId,totalFlow,uploadTime,userCode,userDownFlow,userGrade,userType,userUpFlow,country,loginCustomerId,mvnoId,orgId,imsi,u_mvnoName,u_orgName,transToHisTime,sidList,backPlankSid,sids.XX.date,sids.XX.imsi,sids.XX.sidList,sids.XX.type,network,buType,powerLeft,province,city,sids.XX.switchCause -q '{\"logoutdatetime\":{\$gte:$yestoday_d_unix,\$lt:$now_d_unix}}' --type=csv -o /data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_history.csv"
echo $v_execute
echo $v_execute > $v_mongo_command
chmod +x $v_mongo_command
. $v_mongo_command >> /data/dumpdata/data_bak/oss_system/t_monitor_term_history/oss_system_t_monitor_term_history.log

###################################删除ODS层数据######################################
v_gp_db='bigdata'
v_gp_ip='10.7.5.16'
v_gp_u='gpadmin'
v_gp_port='5432'

v_gp_str="delete from ods.ods_oss_system_t_monitor_term_history where logoutdatetime>=$yestoday_d_unix and logoutdatetime<$now_d_unix;"
echo $v_gp_str
v_logoutdatetime_gp="`su - gpadmin -c \"psql -h ${v_gp_ip} -p ${v_gp_port} -U ${v_gp_u} -d ${v_gp_db} -c \\\"${v_gp_str}\\\"\" | sed -n '3p'`"

###################################gpload 第三步装数#################################

gpload -f /data/dumpdata/data_bak/oss_system/t_monitor_term_history/oss_system_t_monitor_term_history.yml >> /data/dumpdata/data_bak/oss_system/t_monitor_term_history/oss_system_t_monitor_term_history.log

#############################greenplum 第四步获取SRC层最大logoutdatetime并更新####################
v_gp_db='bigdata'
v_gp_ip='10.7.5.16'
v_gp_u='gpadmin'
v_gp_port='5432'

v_gp_str="SELECT max(logoutdatetime) as logoutdatetime from src.src_oss_system_t_monitor_term_history;"
v_logoutdatetime_gp="`su - gpadmin -c \"psql -h ${v_gp_ip} -p ${v_gp_port} -U ${v_gp_u} -d ${v_gp_db} -c \\\"${v_gp_str}\\\"\" | sed -n '3p'`"
v_logoutdatetime="$v_logoutdatetime_gp"
v_datetime=`date -d today +"%Y-%m-%d %T"`
echo $v_logoutdatetime
if [ $v_logoutdatetime ] ; then
        v_update_time="UPDATE t_monitor_term_history SET maxtime=$v_logoutdatetime,updatetime='${v_datetime}' WHERE table_name='t_monitor_term_history';"
        mysql -A ${v_mysql_db} -h ${v_mysql_ip} -u${v_mysql_u} -p${v_mysql_p} -e "${v_update_time}"
        echo "Update logoutdatetime"
fi

#############################greenplum 第五步调用src_ods存错过程#################################
v_gp_stored="SELECT ods.ods_all_supplementary();"
v_gp_stored_execute="`su - gpadmin -c \"psql -h ${v_gp_ip} -p ${v_gp_port} -U ${v_gp_u} -d ${v_gp_db} -c \\\"${v_gp_stored}\\\"\" | sed -n '3p'`"
v_gp_stored_t_monitor_term_history="$v_gp_stored_execute"
echo $v_gp_stored_t_monitor_term_history >> /data/dumpdata/data_bak/oss_system/t_monitor_term_history/oss_system_t_monitor_term_history.log

v_datestamp_end=`date +%Y%m%d%H%M%S`
echo "结束时间:"$v_datestamp_end
###################################################保留最近5天的数据########################################################
find /data/dumpdata/data_bak/oss_system/t_monitor_term_history/t_monitor_term_history_bak/ -mtime +2 -name *.csv -exec rm -rf {} \;
###########################mysql###############################
v_mysql_db='datasupport'
v_mysql_ip='10.7.4.79'
v_mysql_u='root'
v_mysql_p='zt9YdPveyA9UDPKJGKJ'
job_name='t_monitor_history'
v_date=`date +%Y-%m-%d`
mysql -A ${v_mysql_db} -h ${v_mysql_ip} -u${v_mysql_u} -p${v_mysql_p} -e "insert into t_job_task(job_name,task_desc,task_status,job_date) values('$job_name','t_monitor_history_job',1,'$v_date');"
